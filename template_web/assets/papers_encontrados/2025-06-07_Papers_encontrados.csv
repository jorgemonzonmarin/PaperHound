"Query","Titulo","DOI","Revista","Abstract"
"query1","Magnesium oxide use and reduced risk of dementia: a retrospective, nationwide cohort study in Taiwan","10.1080/03007995.2017.1385449","Scopus","  We have studied the structure of $^4$He droplets doped with magnesium atoms using density functional theory. We have found that the solvation properties of this system strongly depend on the size of the $^4$He droplet. For small drops, Mg resides in a deep surface state, whereas for large size drops it is fully solvated but radially delocalized in their interior. We have studied the $3s3p$ $^1$P$_1 \leftarrow 3s^2$ $^1$S$_0$ transition of the dopant, and have compared our results with experimental data from laser induced fluorescence (LIF). Line broadening effects due to the coupling of dynamical deformations of the surrounding helium with the dipole excitation of the impurity are explicitly taken into account. We show that the Mg radial delocalization inside large droplets may help reconcile the apparently contradictory solvation properties of magnesium as provided by LIF and electron-impact ionization experiments. The structure of $^4$He drops doped with two magnesium atoms is also studied and used to interpret the results of resonant two-photon-ionization (R2PI) and LIF experiments. We have found that the two solvated Mg atoms do not easily merge into a dimer, but rather form a weakly-bound state due to the presence of an energy barrier caused by the helium environment that keep them some 9.5 \AA{} apart, preventing the formation of the Mg$_2$ molecule. From this observation, we suggest that Mg atoms in $^4$He drops may form, under suitable conditions, a soft ``foam''-like aggregate rather than coalesce into a compact metallic cluster. Our findings are in qualitative agreement with recent R2PI experimental evidences. We predict that, contrarily, Mg atoms adsorbed in $^3$He droplets do not form such metastable aggregates. "
"query1","How different is the care of terminal pancreatic cancer patients in inpatient palliative care units and acute hospital wards? A nationwide population-based study Cancer palliative care","10.1186/s12904-016-0075-x","Scopus","  In this paper we introduce novel Virtual Reality (VR) and Augmented Reality (AR) treatments to improve the psychological well being of patients in palliative care, based on interviews with a clinical psychologist who has successfully implemented VR assisted interventions on palliative care patients in the Hong Kong hospital system. Our VR and AR assisted interventions are adaptations of traditional palliative care therapies which simultaneously facilitate patients communication with family and friends while isolated in hospital due to physical weakness and COVID-19 related restrictions. The first system we propose is a networked, metaverse platform for palliative care patients to create customized virtual environments with therapists, family and friends which function as immersive and collaborative versions of 'life review' and 'reminiscence therapy'. The second proposed system will investigate the use of Mixed Reality telepresence and haptic touch in an AR environment, which will allow palliative care patients to physically feel friends and family in a virtual space, adding to the sense of presence and immersion in that environment. "
"query 1","Algorithms for computing closest points for segments","10.1016/j.comgeo.2025.102196","Scopus","  Given a set $P$ of $n$ points and a set $S$ of $n$ segments in the plane, we consider the problem of computing for each segment of $S$ its closest point in $P$. The previously best algorithm solves the problem in $n^{4/3}2^{O(\log^*n)}$ time [Bespamyatnikh, 2003] and a lower bound (under a somewhat restricted model) $\Omega(n^{4/3})$ has also been proved. In this paper, we present an $O(n^{4/3})$ time algorithm and thus solve the problem optimally (under the restricted model). In addition, we also present data structures for solving the online version of the problem, i.e., given a query segment (or a line as a special case), find its closest point in $P$. Our new results improve the previous work. "
"query 1","Autoscaling of microservice resources based on dense connectivity spatio-temporal GNN and Q-learning","10.1016/j.future.2025.107909","Scopus","  As cloud applications shift from monoliths to loosely coupled microservices, application developers must decide how many compute resources (e.g., number of replicated containers) to assign to each microservice within an application. This decision affects both (1) the dollar cost to the application developer and (2) the end-to-end latency perceived by the application user. Today, individual microservices are autoscaled independently by adding VMs whenever per-microservice CPU or memory utilization crosses a configurable threshold. However, an application user's end-to-end latency consists of time spent on multiple microservices and each microservice might need a different number of VMs to achieve an overall end-to-end latency.   We present COLA, an autoscaler for microservice-based applications, which collectively allocates VMs to microservices with a global goal of minimizing dollar cost while keeping end-to-end application latency under a given target. Using 5 open-source applications, we compared COLA to several utilization and machine learning based autoscalers. We evaluate COLA across different compute settings on Google Kubernetes Engine (GKE) in which users manage compute resources, GKE standard, and a new mode of operation in which the cloud provider manages compute infrastructure, GKE Autopilot. COLA meets a desired median or tail latency target on 53 of 63 workloads where it provides a cost reduction of 19.3%, on average, over the next cheapest autoscaler. COLA is the most cost effective autoscaling policy for 48 of these 53 workloads. The cost savings from managing a cluster with COLA result in COLA paying for its training cost in a few days. On smaller applications, for which we can exhaustively search microservice configurations, we find that COLA is optimal for 90% of cases and near optimal otherwise. "
"query 1","Analysis of Student Achievement in the Context of Semantic Web Application in English Education","10.22399/ijcesen.1371","Scopus","  Web 3.0 is an evolving extension of the web 2.0 scenario. The perceptions regarding web 3.0 is different from person to person . Web 3.0 Architecture supports ubiquitous connectivity, network computing, open identity, intelligent web, distributed databases and intelligent applications. Some of the technologies which lead to the design and development of web 3.0 applications are Artificial intelligence, Automated reasoning, Cognitive architecture, Semantic web . An attempt is made to capture the requirements of Students inline with web 3.0 so as to bridge the gap between the design and development of web 3.0 applications and requirements among Students. Maximum Spanning Tree modeling of the requirements facilitate the identification of key areas and key attributes in the design and development of software products for Students in Web 3.0 using Discriminant analysis. Keywords : Web 3.0, Discriminant analysis, Design and Development, Model, Maximum Spanning Tree 1. "
"query 1","Unveiling the relationships between language learning strategies and academic achievement among Moroccan EFL university students","10.32674/4b63m946","Scopus","  Previous studies have shown that enhanced student assessment literacy can lead to improvements in academic performance in EFL (English as a Foreign Language) writing. Additionally, psychological factors such as self-efficacy, achievement motivation, and writing anxiety significantly influence EFL writing outcomes. However, the relationship between student writing assessment literacy (SWAL) and these psychological factors remains unclear. The present study aims to explore how SWAL affects psychological factors in the Chinese EFL context. Data were collected from 103 Chinese undergraduate EFL students using four questionnaires: the Student Writing Assessment Literacy Scale (SWAL), the Self-Efficacy for Writing Scale (SEWS), the Achievement Goal Questionnaire (AGQ), and the Second Language Writing Anxiety Inventory (SLWAI). Ordinal logistic regression was employed to analyze the data. The results indicated that higher levels of SWAL were positively associated with writing self-efficacy and achievement motivation, while negatively related to writing anxiety. These findings have significant pedagogical implications for second language (L2) writing instructions, emphasizing the importance of integrating SWAL training into writing instruction to enhance students' writing experiences and outcomes. "
"query 1","Research on multiple enhanced k combination reverse Skyline query method","10.1186/s40537-025-01076-y","Scopus","  Skyline queries enable multi-criteria optimization by filtering objects that are worse in all the attributes of interest than another object. To handle the large answer set of skyline queries in high-dimensional datasets, the concept of k-dominance was proposed where an object is said to dominate another object if it is better (or equal) in at least k attributes. This relaxes the full domination criterion of normal skyline queries and, therefore, produces lesser number of skyline objects. This is called the k-dominant skyline set. Many practical applications, however, require that the preferences are applied on a joined relation. Common examples include flights having one or multiple stops, a combination of product price and shipping costs, etc. In this paper, we extend the k-dominant skyline queries to the join paradigm by enabling such queries to be asked on joined relations. We call such queries KSJQ (k-dominant skyline join queries). The number of skyline attributes, k, that an object must dominate is from the combined set of skyline attributes of the joined relation. We show how pre-processing the base relations helps in reducing the time of answering such queries over the naive method of joining the relations first and then running the k-dominant skyline computation. We also extend the query to handle cases where the skyline preference is on aggregated values in the joined relation (such as total cost of the multiple legs of the flight) which are available only after the join is performed. In addition to these problems, we devise efficient algorithms to choose the value of k based on the desired cardinality of the final skyline set. Experiments on both real and synthetic datasets demonstrate the efficiency, scalability and practicality of our algorithms. "
"query 1","Query optimization","https://dl.acm.org/doi/pdf/10.1145/234313.234367","ACM Computing Surveys (CSUR)","  The handling of user preferences is becoming an increasingly important issue in present-day information systems. Among others, preferences are used for information filtering and extraction to reduce the volume of data presented to the user. They are also used to keep track of user profiles and formulate policies to improve and automate decision making.   We propose here a simple, logical framework for formulating preferences as preference formulas. The framework does not impose any restrictions on the preference relations and allows arbitrary operation and predicate signatures in preference formulas. It also makes the composition of preference relations straightforward. We propose a simple, natural embedding of preference formulas into relational algebra (and SQL) through a single winnow operator parameterized by a preference formula. The embedding makes possible the formulation of complex preference queries, e.g., involving aggregation, by piggybacking on existing SQL constructs. It also leads in a natural way to the definition of further, preference-related concepts like ranking. Finally, we present general algebraic laws governing the winnow operator and its interaction with other relational algebra operators. The preconditions on the applicability of the laws are captured by logical formulas. The laws provide a formal foundation for the algebraic optimization of preference queries. We demonstrate the usefulness of our approach through numerous examples. "
"query 1","Query processing in a system for distributed databases (SDD-1)","https://dl.acm.org/doi/abs/10.1145/319628.319650","ACM Transactions on …","  Performance-critical industrial applications, including large-scale program, network, and distributed system analyses, are increasingly reliant on recursive queries for data analysis. Yet traditional relational algebra-based query optimization techniques do not scale well to recursive query processing due to the iterative nature of query evaluation, where relation cardinalities can change unpredictably during the course of a single query execution. To avoid error-prone cardinality estimation, adaptive query processing techniques use runtime information to inform query optimization, but these systems are not optimized for the specific needs of recursive query processing. In this paper, we introduce Adaptive Metaprogramming, an innovative technique that shifts recursive query optimization and code generation from compile-time to runtime using principled metaprogramming, enabling dynamic optimization and re-optimization before and after query execution has begun. We present a custom join-ordering optimization applicable at multiple stages during query compilation and execution. Through Carac, a custom Datalog engine, we evaluate the optimization potential of Adaptive Metaprogramming and show unoptimized recursive query execution time can be improved by three orders of magnitude and hand-optimized queries by 6x. "
"query 1","Queries revisited","https://www.sciencedirect.com/science/article/pii/S030439750300608X","Theoretical Computer Science","  We revisit the problem of finding shortest unique substring (SUS) proposed recently by [6]. We propose an optimal $O(n)$ time and space algorithm that can find an SUS for every location of a string of size $n$. Our algorithm significantly improves the $O(n^2)$ time complexity needed by [6]. We also support finding all the SUSes covering every location, whereas the solution in [6] can find only one SUS for every location. Further, our solution is simpler and easier to implement and can also be more space efficient in practice, since we only use the inverse suffix array and longest common prefix array of the string, while the algorithm in [6] uses the suffix tree of the string and other auxiliary data structures. Our theoretical results are validated by an empirical study that shows our algorithm is much faster and more space-saving than the one in [6]. "
"query 1","An overview of query optimization in relational systems","https://dl.acm.org/doi/abs/10.1145/275487.275492","Proceedings of the seventeenth ACM SIGACT …","  Most present day organisations make use of some automated information system. This usually means that a large body of vital corporate information is stored in these information systems. As a result, an essential function of information systems should be the support of disclosure of this information.   We purposely use the term {\em information disclosure} in this context. When using the term information disclosure we envision a computer supported mechanism that allows for an easy and intuitive formulation of queries in a language that is as close to the user's perception of the universe of discourse as possible.   From this point of view, it is only obvious that we do not consider a simple query mechanism where users have to enter complex queries manually and look up what information is stored in a set of relational tables. Without a set of adequate information disclosure avenues an information system becomes worthless since there is no use in storing information that will never be retrieved. "
"query 1","Query enrichment for web-query classification","https://dl.acm.org/doi/abs/10.1145/1165774.1165776","ACM Transactions on …","  Human labeling of data can be very time-consuming and expensive, yet, in many cases it is critical for the success of the learning process. In order to minimize human labeling efforts, we propose a novel active learning solution that does not rely on existing sources of unlabeled data. It uses a small amount of labeled data as the core set for the synthesis of useful membership queries (MQs) - unlabeled instances generated by an algorithm for human labeling. Our solution uses modification operators, functions that modify instances to some extent. We apply the operators on a small set of instances (core set), creating a set of new membership queries. Using this framework, we look at the instance space as a search space and apply search algorithms in order to generate new examples highly relevant to the learner. We implement this framework in the textual domain and test it on several text classification tasks and show improved classifier performance as more MQs are labeled and incorporated into the training set. To the best of our knowledge, this is the first work on membership queries in the textual domain. "
"query 1","Query processing in a system for distributed databases (SDD-1)","https://doi.org/10.1145/319628.319650","OpenAlex","  Performance-critical industrial applications, including large-scale program, network, and distributed system analyses, are increasingly reliant on recursive queries for data analysis. Yet traditional relational algebra-based query optimization techniques do not scale well to recursive query processing due to the iterative nature of query evaluation, where relation cardinalities can change unpredictably during the course of a single query execution. To avoid error-prone cardinality estimation, adaptive query processing techniques use runtime information to inform query optimization, but these systems are not optimized for the specific needs of recursive query processing. In this paper, we introduce Adaptive Metaprogramming, an innovative technique that shifts recursive query optimization and code generation from compile-time to runtime using principled metaprogramming, enabling dynamic optimization and re-optimization before and after query execution has begun. We present a custom join-ordering optimization applicable at multiple stages during query compilation and execution. Through Carac, a custom Datalog engine, we evaluate the optimization potential of Adaptive Metaprogramming and show unoptimized recursive query execution time can be improved by three orders of magnitude and hand-optimized queries by 6x. "
"query 1","Metabolomic database annotations via query of elemental compositions: Mass accuracy is insufficient even at less than 1 ppm","https://doi.org/10.1186/1471-2105-7-234","OpenAlex","  Systematic variation is a common issue in metabolomics data analysis. Therefore, different scaling and normalization techniques are used to preprocess the data for metabolomics data analysis. Although several scaling methods are available in the literature, however, choice of scaling, transformation and/or normalization technique influence the further statistical analysis. It is challenging to choose the appropriate scaling technique for downstream analysis to get accurate results or to make a proper decision. Moreover, the existing scaling techniques are sensitive to outliers or extreme values. To fill the gap, our objective is to introduce a robust scaling approach that is not influenced by outliers as well as provides more accurate results for downstream analysis. Here, we introduced a new weighted scaling approach that is robust against outliers however, where no additional outlier detection/treatment step is needed in data preprocessing and also compared it with the conventional scaling and normalization techniques through artificial and real metabolomics datasets. We evaluated the performance of the proposed method in comparison to the other existing conventional scaling techniques using metabolomics data analysis in both the absence and presence of different percentages of outliers. Results show that in most cases, the proposed scaling technique performs better than the traditional scaling methods in both the absence and presence of outliers. The proposed method improves the further downstream metabolomics analysis. The R function of the proposed robust scaling method is available at https://github.com/nishithkumarpaul/robustScaling/blob/main/wscaling.R "
"query 1","Query 1: Inadmissible Random Assignments","https://doi.org/10.2307/1266750","OpenAlex","  Video moment retrieval aims at finding the start and end timestamps of a moment (part of a video) described by a given natural language query. Fully supervised methods need complete temporal boundary annotations to achieve promising results, which is costly since the annotator needs to watch the whole moment. Weakly supervised methods only rely on the paired video and query, but the performance is relatively poor. In this paper, we look closer into the annotation process and propose a new paradigm called ""glance annotation"". This paradigm requires the timestamp of only one single random frame, which we refer to as a ""glance"", within the temporal boundary of the fully supervised counterpart. We argue this is beneficial because comparing to weak supervision, trivial cost is added yet more potential in performance is provided. Under the glance annotation setting, we propose a method named as Video moment retrieval via Glance Annotation (ViGA) based on contrastive learning. ViGA cuts the input video into clips and contrasts between clips and queries, in which glance guided Gaussian distributed weights are assigned to all clips. Our extensive experiments indicate that ViGA achieves better results than the state-of-the-art weakly supervised methods by a large margin, even comparable to fully supervised methods in some cases. "
"query 1","Stochastic matching with few queries: (1-ε) approximation","https://doi.org/10.1145/3357713.3384340","OpenAlex","  In this paper, we study the weighted stochastic matching problem. Let $G=(V, E)$ be a given edge-weighted graph and let its realization $\mathcal{G}$ be a random subgraph of $G$ that includes each edge $e\in E$ independently with a known probability $p_e$. The goal in this problem is to pick a sparse subgraph $Q$ of $G$ without prior knowledge of $G$'s realization, such that the maximum weight matching among the realized edges of $Q$ (i.e. the subgraph $Q\cap \mathcal{G}$) in expectation approximates the maximum weight matching of the entire realization $\mathcal{G}$.   Attaining any constant approximation ratio for this problem requires selecting a subgraph of max-degree $\Omega(1/p)$ where $p=\min_{e\in E} p_e$. On the positive side, there exists a $(1-\epsilon)$-approximation algorithm by Behnezhad and Derakhshan, albeit at the cost of max-degree having exponential dependence on $1/p$. Within the $\text{poly}(1/p)$ regime, however, the best-known algorithm achieves a $0.536$ approximation ratio due to Dughmi, Kalayci, and Patel improving over the $0.501$ approximation algorithm by Behnezhad, Farhadi, Hajiaghayi, and Reyhani.   In this work, we present a 0.68 approximation algorithm with $O(1/p)$ queries per vertex, which is asymptotically tight. This is even an improvement over the best-known approximation ratio of $2/3$ for unweighted graphs within the $\text{poly}(1/p)$ regime due to Assadi and Bernstein. The $2/3$ approximation ratio is proven tight in the presence of a few correlated edges in $\mathcal{G}$, indicating that surpassing the $2/3$ barrier should rely on the independent realization of edges. Our analysis involves reducing the problem to designing a randomized matching algorithm on a given stochastic graph with some variance-bounding properties. "
"query 1","Notes on Queries in Ethnography<sup>1</sup>","https://doi.org/10.1525/aa.1964.66.3.02a00860","OpenAlex","  Wavelets are a useful basis for constructing solutions of the integral and differential equations of scattering theory. Wavelet bases efficiently represent functions with smooth structures on different scales, and the matrix representation of operators in a wavelet basis are well-approximated by sparse matrices. The basis functions are related to solutions of a linear renormalization group equation, and the basis functions have structure on all scales. Numerical methods based on this renormalization group equation are discussed. These methods lead to accurate and efficient numerical approximations to the scattering equations. These notes provide a detailed introduction to the subject that focuses on numerical methods. We plan to provide periodic updates to these notes. "
"query 1","Query Order","http://arxiv.org/abs/cs/9909020v1","ArXiv","  We formalize and study the problem of repairing database queries based on user feedback in the form of a collection of labeled examples. We propose a framework based on the notion of a proximity pre-order, and we investigate and compare query repairs for conjunctive queries (CQs) using different such pre-orders. The proximity pre-orders we consider are based on query containment and on distance metrics for CQs. "
"query 1","Multi-query quantum sums","http://arxiv.org/abs/1107.1940v1","ArXiv","  We define a universal state sum construction which specializes to most previously known state sums (Turaev-Viro, Dijkgraaf-Witten, Crane-Yetter, Douglas-Reutter, Witten-Reshetikhin-Turaev surgery formula, Brown-Arf). The input data for the state sum is an n-category satisfying various conditions, including finiteness, semisimplicity and n-pivotality. From this n-category one constructs an n+1-dimensional TQFT, and applying the TQFT gluing rules to a handle decomposition of an n+1-manifold produces the state sum. "
"query 1","Succinct Approximate Rank Queries","http://arxiv.org/abs/1704.07710v1","ArXiv","  We consider the problem of summarizing a multi set of elements in $\{1, 2, \ldots , n\}$ under the constraint that no element appears more than $\ell$ times. The goal is then to answer \emph{rank} queries --- given $i\in\{1, 2, \ldots , n\}$, how many elements in the multi set are smaller than $i$? --- with an additive error of at most $\Delta$ and in constant time. For this problem, we prove a lower bound of $\mathcal B_{\ell,n,\Delta}\triangleq$ $\left\lfloor{\frac{n}{\left\lceil{\Delta / \ell}\right\rceil}}\right\rfloor $ $\log\big({\max\{\left\lfloor{\ell / \Delta}\right\rfloor,1\} + 1}\big)$ bits and provide a \emph{succinct} construction that uses $\mathcal B_{\ell,n,\Delta}(1+o(1))$ bits. Next, we generalize our data structure to support processing of a stream of integers in $\{0,1,\ldots,\ell\}$, where upon a query for some $i\le n$ we provide a $\Delta$-additive approximation for the sum of the \emph{last} $i$ elements. We show that this too can be done using $\mathcal B_{\ell,n,\Delta}(1+o(1))$ bits and in constant time. This yields the first sub linear space algorithm that computes approximate sliding window sums in $O(1)$ time, where the window size is given at the query time; additionally, it requires only $(1+o(1))$ more space than is needed for a fixed window size. "
"query 1","Quantum Bounded Query Complexity","http://arxiv.org/abs/quant-ph/9903035v1","ArXiv","  We show that any boolean function can be evaluated optimally by a quantum query algorithm that alternates a certain fixed, input-independent reflection with a second reflection that coherently queries the input string. Originally introduced for solving the unstructured search problem, this two-reflections structure is therefore a universal feature of quantum algorithms.   Our proof goes via the general adversary bound, a semi-definite program (SDP) that lower-bounds the quantum query complexity of a function. By a quantum algorithm for evaluating span programs, this lower bound is known to be tight up to a sub-logarithmic factor. The extra factor comes from converting a continuous-time query algorithm into a discrete-query algorithm. We give a direct and simplified quantum algorithm based on the dual SDP, with a bounded-error query complexity that matches the general adversary bound.   Therefore, the general adversary lower bound is tight; it is in fact an SDP for quantum query complexity. This implies that the quantum query complexity of the composition f(g,...,g) of two boolean functions f and g matches the product of the query complexities of f and g, without a logarithmic factor for error reduction. It further shows that span programs are equivalent to quantum query algorithms. "
"query 1","Approximate Polytope Membership Queries","http://arxiv.org/abs/1604.01183v2","ArXiv","  In the polytope membership problem, a convex polytope $K$ in $\mathbb{R}^d$ is given, and the objective is to preprocess $K$ into a data structure so that, given any query point $q \in \mathbb{R}^d$, it is possible to determine efficiently whether $q \in K$. We consider this problem in an approximate setting. Given an approximation parameter $\varepsilon$, the query can be answered either way if the distance from $q$ to $K$'s boundary is at most $\varepsilon$ times $K$'s diameter. We assume that the dimension $d$ is fixed, and $K$ is presented as the intersection of $n$ halfspaces. Previous solutions to approximate polytope membership were based on straightforward applications of classic polytope approximation techniques by Dudley (1974) and Bentley et al. (1982). The former is optimal in the worst-case with respect to space, and the latter is optimal with respect to query time.   We present four main results. First, we show how to combine the two above techniques to obtain a simple space-time trade-off. Second, we present an algorithm that dramatically improves this trade-off. In particular, for any constant $\alpha \ge 4$, this data structure achieves query time $O(1/\varepsilon^{(d-1)/\alpha})$ and space roughly $O(1/\varepsilon^{(d-1)(1 - O(\log \alpha)/\alpha)})$. We do not know whether this space bound is tight, but our third result shows that there is a convex body such that our algorithm achieves a space of at least $\Omega( 1/\varepsilon^{(d-1)(1-O(\sqrt{\alpha})/\alpha} )$. Our fourth result shows that it is possible to reduce approximate Euclidean nearest neighbor searching to approximate polytope membership queries. Combined with the above results, this provides significant improvements to the best known space-time trade-offs for approximate nearest neighbor searching in $\mathbb{R}^d$. "
"query 1","CO Query, Content-Only Query","10.1007/springerreference_64569","CrossRef","  This paper aims to tackle the challenging problem of one-shot object detection. Given a query image patch whose class label is not included in the training data, the goal of the task is to detect all instances of the same class in a target image. To this end, we develop a novel {\em co-attention and co-excitation} (CoAE) framework that makes contributions in three key technical aspects. First, we propose to use the non-local operation to explore the co-attention embodied in each query-target pair and yield region proposals accounting for the one-shot situation. Second, we formulate a squeeze-and-co-excitation scheme that can adaptively emphasize correlated feature channels to help uncover relevant proposals and eventually the target objects. Third, we design a margin-based ranking loss for implicitly learning a metric to predict the similarity of a region proposal to the underlying query, no matter its class label is seen or unseen in training. The resulting model is therefore a two-stage detector that yields a strong baseline on both VOC and MS-COCO under one-shot setting of detecting objects from both seen and never-seen classes. Codes are available at https://github.com/timy90022/One-Shot-Object-Detection. "
"query 1","Using Query Store for Query Performance and Execution Plans","10.1007/978-1-4842-8891-7_6","CrossRef","  Recursive queries and recursive derived tables constitute an important part of the SQL standard. Their efficient processing is important for many real-life applications that rely on graph or hierarchy traversal. Position-enabled column-stores offer a novel opportunity to improve run times for this type of queries. Such systems allow the engine to explicitly use data positions (row ids) inside its core and thus, enable novel efficient implementations of query plan operators.   In this paper, we present an approach that significantly speeds up recursive query processing inside RDBMSes. Its core idea is to employ a particular aspect of column-store technology (late materialization) which enables the query engine to manipulate data positions during query execution. Based on it, we propose two sets of Volcano-style operators intended to process different query cases.   In order validate our ideas, we have implemented the proposed approach in PosDB, an RDBMS column-store with SQL support. We experimentally demonstrate the viability of our approach by providing a comparison with PostgreSQL. Experiments show that for breadth-first search: 1) our position-based approach yields up to 6x better results than PostgreSQL, 2) our tuple-based one results in only 3x improvement when using a special rewriting technique, but it can work in a larger number of cases, and 3) both approaches can't be emulated in row-stores efficiently. "
"query 1","Figure 2: (A) Skyline query with certain data (B) Skyline query with uncertain data (C) Constrained skyline query with certain data (D) Constrained skyline query with uncertain data (E) Constrained skyline query with uncertain data and <i>MBR</i>.","10.7717/peerj-cs.2225/fig-2","CrossRef","  Understanding the influence of a product is crucially important for making informed business decisions. This paper introduces a new type of skyline queries, called uncertain reverse skyline, for measuring the influence of a probabilistic product in uncertain data settings. More specifically, given a dataset of probabilistic products P and a set of customers C, an uncertain reverse skyline of a probabilistic product q retrieves all customers c in C which include q as one of their preferred products. We present efficient pruning ideas and techniques for processing the uncertain reverse skyline query of a probabilistic product using R-Tree data index. We also present an efficient parallel approach to compute the uncertain reverse skyline and influence score of a probabilistic product. Our approach significantly outperforms the baseline approach derived from the existing literature. The efficiency of our approach is demonstrated by conducting extensive experiments with both real and synthetic datasets. "
"query 1","Figure 5: SPARQL query results for Query 30.","10.7717/peerj.16087/fig-5","CrossRef","  We introduce LEAF-QA, a comprehensive dataset of $250,000$ densely annotated figures/charts, constructed from real-world open data sources, along with ~2 million question-answer (QA) pairs querying the structure and semantics of these charts. LEAF-QA highlights the problem of multimodal QA, which is notably different from conventional visual QA (VQA), and has recently gained interest in the community. Furthermore, LEAF-QA is significantly more complex than previous attempts at chart QA, viz. FigureQA and DVQA, which present only limited variations in chart data. LEAF-QA being constructed from real-world sources, requires a novel architecture to enable question answering. To this end, LEAF-Net, a deep architecture involving chart element localization, question and answer encoding in terms of chart elements, and an attention network is proposed. Different experiments are conducted to demonstrate the challenges of QA on LEAF-QA. The proposed architecture, LEAF-Net also considerably advances the current state-of-the-art on FigureQA and DVQA. "
"query 1","Query Recompilation","10.1007/978-1-4842-3888-2_18","CrossRef","  We study the effect of query order on computational power, and show that $\pjk$-the languages computable via a polynomial-time machine given one query to the jth level of the boolean hierarchy followed by one query to the kth level of the boolean hierarchy-equals $\redttnp{j+2k-1}$ if j is even and k is odd, and equals $\redttnp{j+2k}$ otherwise. Thus, unless the polynomial hierarchy collapses, it holds that for each $1\leq j \leq k$: $\pjk = \pkj \iff (j=k) \lor (j{is even} \land k=j+1)$. We extend our analysis to apply to more general query classes. "
